{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn import preprocessing as scale\n",
    "from sklearn.model_selection import train_test_split\n",
    "from tqdm.notebook import tqdm\n",
    "from sklearn import mixture\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.decomposition import PCA\n",
    "\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.svm import SVC, LinearSVC\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import classification_report\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import accuracy_score, confusion_matrix, recall_score, f1_score, classification_report\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_file = [\"a.csv\" , \"b.csv\" , \"c.csv\" , \"e.csv\" ]\n",
    "columns_to_drop_bi = ['proto', 'ip_src', 'ip_dst']\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The Federated Learner:\n",
    "\n",
    "Parameters:\n",
    "\n",
    "x: The data to be decided (array)\n",
    "\n",
    "clfs:  The binary classifiers;(List of Classifiers)\n",
    "\n",
    "p_nodes: The proportion of the nodes;(List)\n",
    "\n",
    "gms: Gassian Mixture Models (List of GMMs)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Binary=RF"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Binary=ANN:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Binary = CNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "    # data = pd.read_csv('openSmile_resampled/overall.csv')\n",
    "\n",
    "    # data = pd.read_csv('openSmile_raw/f.csv')\n",
    "\n",
    "    data = pd.read_csv('openSmile_balanced/f.csv')\n",
    "\n",
    "    # Split the data into features (X) and labels (y)\n",
    "    X = data.drop('y', axis=1)\n",
    "    y = data['y']\n",
    "\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "    n_estimators_custom = 100\n",
    "    max_depth_custom = 10\n",
    "    min_samples_split_custom = 5\n",
    "    criterion_custom = 'gini'\n",
    "\n",
    "    clf = RandomForestClassifier(n_estimators=n_estimators_custom,\n",
    "                                   max_depth=max_depth_custom,\n",
    "                                   min_samples_split=min_samples_split_custom,\n",
    "                                   criterion=criterion_custom,\n",
    "                                   random_state=42)\n",
    "    clf.fit(X_train, y_train)\n",
    "    y_pred = clf.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Centralized_RF_learn\n",
    "\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "def Centralized_RF_learn(input_file, columns_to_drop_bi, random_state,  test_size = 0.25):\n",
    "    \n",
    "    data_list = []\n",
    "    for i in range(len(input_file)):    \n",
    "    data = pd.read_csv(input_file[i])\n",
    "        data.drop(columns = columns_to_drop_bi, inplace = True)\n",
    "        data_list.append(data)\n",
    "\n",
    "    rs = random_state\n",
    "\n",
    "    n_samples = []\n",
    "    x_train_list = []\n",
    "    y_train_list = []\n",
    "    x_test_list = []\n",
    "    y_test_list = []\n",
    "    y = [np.ones(len(data_list[i])) * (i + 1) for i in range(len(data_list))]\n",
    "    for i in range(len(input_file)): # reading the data\n",
    "        data0 = pd.read_csv(input_file[i])\n",
    "        y[i][data0.is_abnormal == 0] = 0\n",
    "        n_samples.append(len(y[i])) # the number of this node\n",
    "        x = data_list[i]\n",
    "        x = x.drop(columns='is_attack')\n",
    "        x_train, x_test, y_train, y_test = train_test_split(x, y[i], test_size = test_size, random_state = rs)\n",
    "        x_train_list.append(x_train)\n",
    "        y_train_list.append(y_train)\n",
    "        x_test_list.append(x_test)\n",
    "        y_test_list.append(y_test)\n",
    "\n",
    "    x_train_full = pd.concat(x_train_list, ignore_index=True)\n",
    "    scaler = scale.StandardScaler().fit(x_train_full)\n",
    "\n",
    "    for i in range(len(input_file)):\n",
    "        x_train_list[i] = scaler.transform(x_train_list[i])\n",
    "\n",
    "    total_n_samples = np.sum(n_samples)\n",
    "\n",
    "    p_nodes =  np.array(n_samples) / total_n_samples\n",
    "\n",
    "\n",
    "    for i in range(len(input_file)):\n",
    "        x_test_list[i] = scaler.transform(x_test_list[i])\n",
    "\n",
    "    x_test = np.vstack(x_test_list)\n",
    "    y_test = np.hstack(y_test_list)\n",
    "    x_train = np.vstack(x_train_list)\n",
    "    y_train = np.hstack(y_train_list)\n",
    "\n",
    "    classifier = KNeighborsClassifier()\n",
    "    classifier.fit(x_train, y_train)\n",
    "\n",
    "    prediction = classifier.predict(x_test)\n",
    "\n",
    "    correct = prediction == y_test\n",
    "    accuracy = np.mean(correct)\n",
    "\n",
    "    accs = []\n",
    "    for i in range(len(input_file)+1):\n",
    "        if len(correct[y_test==i]) == 0:\n",
    "            accs.append(0)\n",
    "        else:\n",
    "            accs.append(np.mean(correct[y_test==i]))\n",
    "\n",
    "    recalls = []\n",
    "    for i in range(len(input_file)+1):\n",
    "        if len(correct[prediction==i]) == 0:\n",
    "            recalls.append(0)\n",
    "        else:\n",
    "            recalls.append(np.mean(correct[prediction==i]))\n",
    "\n",
    "    return recalls, accs \n",
    "\n",
    "\n",
    "\n",
    "def test_RF_centralized(epoch, input_file, columns_to_drop_bi):\n",
    "\n",
    "   recall , precision = np.ones((epoch,len(input_file)+1))*0 , np.ones((epoch,len(input_file)+1))*0\n",
    "\n",
    "   print('main process:')\n",
    "\n",
    "   for rs in tqdm(range(epoch)):\n",
    "      recalls, accs = Centralized_knn_learn(input_file = input_file, columns_to_drop_bi=columns_to_drop_bi, random_state=rs)\n",
    "      recall[rs,:] = recalls\n",
    "      precision[rs,:] = accs\n",
    "\n",
    "   for i in range(len(input_file)+1):\n",
    "      print('Precision mean=',np.mean(precision[:,i]))\n",
    "      print('Precision std=',np.std(precision[:,i]))\n",
    "\n",
    "   for i in range(len(input_file)+1):\n",
    "      print('Recall mean of ',i,' =',np.mean(recall[:,i]))\n",
    "      print('Recall std of ',i,' =',np.std(recall[:,i]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "main process:\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8c37c794aabb47239f8395d34da14ed1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(HTML(value=''), FloatProgress(value=0.0, max=1.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Precision mean= 0.9986343062275636\n",
      "Precision std= 0.0\n",
      "Precision mean= 1.0\n",
      "Precision std= 0.0\n",
      "Precision mean= 1.0\n",
      "Precision std= 0.0\n",
      "Precision mean= 1.0\n",
      "Precision std= 0.0\n",
      "Precision mean= 0.9801434087148373\n",
      "Precision std= 0.0\n",
      "Recall mean of  0  = 0.9971946230274693\n",
      "Recall std of  0  = 0.0\n",
      "Recall mean of  1  = 1.0\n",
      "Recall std of  1  = 0.0\n",
      "Recall mean of  2  = 1.0\n",
      "Recall std of  2  = 0.0\n",
      "Recall mean of  3  = 0.999599278701663\n",
      "Recall std of  3  = 0.0\n",
      "Recall mean of  4  = 0.9908001115137999\n",
      "Recall std of  4  = 0.0\n"
     ]
    }
   ],
   "source": [
    "test_RF_centralized(epoch=1, input_file=input_file, columns_to_drop_bi=columns_to_drop_bi)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Centralized_ANN_learn\n",
    "\n",
    "def Centralized_ANN_learn(input_file, columns_to_drop_bi, random_state,  test_size = 0.25):\n",
    "    \n",
    "    data_list = []\n",
    "    for i in range(len(input_file)):\n",
    "        data = pd.read_csv(input_file[i])\n",
    "        data.drop(columns = columns_to_drop_bi, inplace = True)\n",
    "        data_list.append(data)\n",
    "\n",
    "    rs = random_state\n",
    "\n",
    "    n_samples = []\n",
    "    x_train_list = []\n",
    "    y_train_list = []\n",
    "    x_test_list = []\n",
    "    y_test_list = []\n",
    "    y = [np.ones(len(data_list[i])) * (i + 1) for i in range(len(data_list))]\n",
    "    for i in range(len(input_file)): # reading the data\n",
    "        data0 = pd.read_csv(input_file[i])\n",
    "        y[i][data0.is_abnormal == 0] = 0\n",
    "        n_samples.append(len(y[i])) # the number of this node\n",
    "        x = data_list[i]\n",
    "        x = x.drop(columns='is_attack')\n",
    "        x_train, x_test, y_train, y_test = train_test_split(x, y[i], test_size = test_size, random_state = rs)\n",
    "        x_train_list.append(x_train)\n",
    "        y_train_list.append(y_train)\n",
    "        x_test_list.append(x_test)\n",
    "        y_test_list.append(y_test)\n",
    "\n",
    "    x_train_full = pd.concat(x_train_list, ignore_index=True)\n",
    "    scaler = scale.StandardScaler().fit(x_train_full)\n",
    "\n",
    "    for i in range(len(input_file)):\n",
    "        x_train_list[i] = scaler.transform(x_train_list[i])\n",
    "\n",
    "    total_n_samples = np.sum(n_samples)\n",
    "\n",
    "    p_nodes =  np.array(n_samples) / total_n_samples\n",
    "\n",
    "\n",
    "    for i in range(len(input_file)):\n",
    "        x_test_list[i] = scaler.transform(x_test_list[i])\n",
    "\n",
    "    x_test = np.vstack(x_test_list)\n",
    "    y_test = np.hstack(y_test_list)\n",
    "    x_train = np.vstack(x_train_list)\n",
    "    y_train = np.hstack(y_train_list)\n",
    "\n",
    "    classifier = SVC(kernel = 'rbf', random_state = random_state + 1, gamma='scale',max_iter=-1, probability=True)\n",
    "    classifier.fit(x_train, y_train)\n",
    "\n",
    "    prediction = classifier.predict(x_test)\n",
    "\n",
    "    correct = prediction == y_test\n",
    "    accuracy = np.mean(correct)\n",
    "\n",
    "    accs = []\n",
    "    for i in range(len(input_file)+1):\n",
    "        if len(correct[y_test==i]) == 0:\n",
    "            accs.append(0)\n",
    "        else:\n",
    "            accs.append(np.mean(correct[y_test==i]))\n",
    "\n",
    "    recalls = []\n",
    "    for i in range(len(input_file)+1):\n",
    "        if len(correct[prediction==i]) == 0:\n",
    "            recalls.append(0)\n",
    "        else:\n",
    "            recalls.append(np.mean(correct[prediction==i]))\n",
    "\n",
    "    return recalls, accs \n",
    "\n",
    "\n",
    "def test_ANN_centralized(epoch, input_file, columns_to_drop_bi):\n",
    "\n",
    "   recall , precision = np.ones((epoch,len(input_file)+1))*0 , np.ones((epoch,len(input_file)+1))*0\n",
    "\n",
    "   print('main process:')\n",
    "\n",
    "   for rs in tqdm(range(epoch)):\n",
    "      recalls, accs = Centralized_SVC_learn(input_file = input_file, columns_to_drop_bi=columns_to_drop_bi, random_state=rs)\n",
    "      recall[rs,:] = recalls\n",
    "      precision[rs,:] = accs\n",
    "\n",
    "   for i in range(len(input_file)+1):\n",
    "      print('Precision mean=',np.mean(precision[:,i]))\n",
    "      print('Precision std=',np.std(precision[:,i]))\n",
    "\n",
    "   for i in range(len(input_file)+1):\n",
    "      print('Recall mean of ',i,' =',np.mean(recall[:,i]))\n",
    "      print('Recall std of ',i,' =',np.std(recall[:,i]))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "main process:\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "39929e80c60f4bd68caa92f3ca6ba01a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(HTML(value=''), FloatProgress(value=0.0, max=1.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Precision mean= 1.0\n",
      "Precision std= 0.0\n",
      "Precision mean= 1.0\n",
      "Precision std= 0.0\n",
      "Precision mean= 0.9907539118065434\n",
      "Precision std= 0.0\n",
      "Precision mean= 1.0\n",
      "Precision std= 0.0\n",
      "Precision mean= 0.9721456150027579\n",
      "Precision std= 0.0\n",
      "Recall mean of  0  = 0.9940653969977891\n",
      "Recall std of  0  = 0.0\n",
      "Recall mean of  1  = 1.0\n",
      "Recall std of  1  = 0.0\n",
      "Recall mean of  2  = 1.0\n",
      "Recall std of  2  = 0.0\n",
      "Recall mean of  3  = 1.0\n",
      "Recall std of  3  = 0.0\n",
      "Recall mean of  4  = 1.0\n",
      "Recall std of  4  = 0.0\n"
     ]
    }
   ],
   "source": [
    "test_ANN_centralized(epoch=1, input_file=input_file, columns_to_drop_bi=columns_to_drop_bi)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Centralized_CNN_learn\n",
    "\n",
    "def Centralized_CNN_learn(input_file, columns_to_drop_bi, random_state,  test_size = 0.25):\n",
    "    \n",
    "    data_list = []\n",
    "    for i in range(len(input_file)):\n",
    "        data = pd.read_csv(input_file[i])\n",
    "        data.drop(columns = columns_to_drop_bi, inplace = True)\n",
    "        data_list.append(data)\n",
    "\n",
    "    rs = random_state\n",
    "\n",
    "    n_samples = []\n",
    "    x_train_list = []\n",
    "    y_train_list = []\n",
    "    x_test_list = []\n",
    "    y_test_list = []\n",
    "    y = [np.ones(len(data_list[i])) * (i + 1) for i in range(len(data_list))]\n",
    "    for i in range(len(input_file)): # reading the data\n",
    "        data0 = pd.read_csv(input_file[i])\n",
    "        y[i][data0.is_abnormal == 0] = 0\n",
    "        n_samples.append(len(y[i])) # the number of this node\n",
    "        x = data_list[i]\n",
    "        x = x.drop(columns='is_attack')\n",
    "        x_train, x_test, y_train, y_test = train_test_split(x, y[i], test_size = test_size, random_state = rs)\n",
    "        x_train_list.append(x_train)\n",
    "        y_train_list.append(y_train)\n",
    "        x_test_list.append(x_test)\n",
    "        y_test_list.append(y_test)\n",
    "\n",
    "    x_train_full = pd.concat(x_train_list, ignore_index=True)\n",
    "    scaler = scale.StandardScaler().fit(x_train_full)\n",
    "\n",
    "    for i in range(len(input_file)):\n",
    "        x_train_list[i] = scaler.transform(x_train_list[i])\n",
    "\n",
    "    total_n_samples = np.sum(n_samples)\n",
    "\n",
    "    p_nodes =  np.array(n_samples) / total_n_samples\n",
    "\n",
    "\n",
    "    for i in range(len(input_file)):\n",
    "        x_test_list[i] = scaler.transform(x_test_list[i])\n",
    "\n",
    "    x_test = np.vstack(x_test_list)\n",
    "    y_test = np.hstack(y_test_list)\n",
    "    x_train = np.vstack(x_train_list)\n",
    "    y_train = np.hstack(y_train_list)\n",
    "\n",
    "    classifier = LogisticRegression(max_iter=100)\n",
    "    classifier.fit(x_train, y_train)\n",
    "\n",
    "    prediction = classifier.predict(x_test)\n",
    "\n",
    "    correct = prediction == y_test\n",
    "    accuracy = np.mean(correct)\n",
    "\n",
    "    accs = []\n",
    "    for i in range(len(input_file)+1):\n",
    "        if len(correct[y_test==i]) == 0:\n",
    "            accs.append(0)\n",
    "        else:\n",
    "            accs.append(np.mean(correct[y_test==i]))\n",
    "\n",
    "    recalls = []\n",
    "    for i in range(len(input_file)+1):\n",
    "        if len(correct[prediction==i]) == 0:\n",
    "            recalls.append(0)\n",
    "        else:\n",
    "            recalls.append(np.mean(correct[prediction==i]))\n",
    "\n",
    "    return recalls, accs \n",
    "\n",
    "\n",
    "\n",
    "def test_CNN_centralized(epoch, input_file, columns_to_drop_bi):\n",
    "\n",
    "   recall , precision = np.ones((epoch,len(input_file)+1))*0 , np.ones((epoch,len(input_file)+1))*0\n",
    "\n",
    "   print('main process:')\n",
    "\n",
    "   for rs in tqdm(range(epoch)):\n",
    "      recalls, accs = Centralized_LR_learn(input_file = input_file, columns_to_drop_bi=columns_to_drop_bi, random_state=rs)\n",
    "      recall[rs,:] = recalls\n",
    "      precision[rs,:] = accs\n",
    "\n",
    "   for i in range(len(input_file)+1):\n",
    "      print('Precision mean=',np.mean(precision[:,i]))\n",
    "      print('Precision std=',np.std(precision[:,i]))\n",
    "\n",
    "   for i in range(len(input_file)+1):\n",
    "      print('Recall mean of ',i,' =',np.mean(recall[:,i]))\n",
    "      print('Recall std of ',i,' =',np.std(recall[:,i]))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "main process:\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "089f85746bdd4628bdbf3dd4d4039bb0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(HTML(value=''), FloatProgress(value=0.0, max=1.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Precision mean= 0.9985172467613548\n",
      "Precision std= 0.0\n",
      "Precision mean= 1.0\n",
      "Precision std= 0.0\n",
      "Precision mean= 0.9932432432432432\n",
      "Precision std= 0.0\n",
      "Precision mean= 1.0\n",
      "Precision std= 0.0\n",
      "Precision mean= 0.9721456150027579\n",
      "Precision std= 0.0\n",
      "Recall mean of  0  = 0.9946361940298507\n",
      "Recall std of  0  = 0.0\n",
      "Recall mean of  1  = 0.9997124784358827\n",
      "Recall std of  1  = 0.0\n",
      "Recall mean of  2  = 0.9937733499377335\n",
      "Recall std of  2  = 0.0\n",
      "Recall mean of  3  = 0.9993990384615384\n",
      "Recall std of  3  = 0.0\n",
      "Recall mean of  4  = 1.0\n",
      "Recall std of  4  = 0.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/irohayachiyo/opt/anaconda3/lib/python3.8/site-packages/sklearn/linear_model/_logistic.py:762: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n"
     ]
    }
   ],
   "source": [
    "test_CNN_centralized(epoch=1, input_file=input_file, columns_to_drop_bi=columns_to_drop_bi)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "4ad6a9ebe595c8604775664ccb6b4edb43e0871577ed89c469f16f6efb723bbd"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
